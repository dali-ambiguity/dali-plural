# Word embeddings.
glove_300d {
  path = glove.840B.300d.txt
  size = 300
}
glove_300d_filtered {
  path = glove.840B.300d.txt.filtered
  size = 300
}
glove_300d_2w {
  path = glove_50_300_2.txt
  size = 300
}
glove_300d_2w_filtered {
  path = glove_50_300_2.txt.filtered
  size = 300
}

# Main configuration.
baseline{
  # Computation limits.
  max_top_antecedents = 250
  max_training_words = 2000
  top_span_ratio = 0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w_filtered}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_features = true
  model_heads = true
  lm_layers = 4
  lm_size = 1024

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = train.english.arrau.plural.jsonlines
  auxiliary_train_path = ""
  train_path_coref = ""
  auxiliary_train_type = ""
  eval_path = dev.english.arrau.plural.jsonlines
  test_path = test.english.arrau.plural.jsonlines
  lm_path = bert_features.hdf5
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
  max_step = 20000
  ntimes_negative_examples=5
  pre_trained_model_path = ""
  use_teacher_annealing = false
}

#first pre train
pre_train_pd_crowd = ${baseline}{
  train_path = english.pd2.0.plural.mv-multi-ant.silver.jsonlines
  eval_path = ""
  test_path = dev.english.arrau.plural.jsonlines
}

#second pre train
pre_train_pd_crowd_element_of = ${baseline}{
  auxiliary_train_type = bridging
  pre_trained_model_path = "../pre_train_pd_crowd/model.max.ckpt"
  train_path = ""
  auxiliary_train_path = train.english.arrau.element.jsonlines
  eval_path = ""
  test_path = dev.english.arrau.plural.jsonlines
}

#after two pre train using annealing on single coref
best = ${baseline}{
  pre_trained_model_path = "../pre_train_pd_crowd_element_of/model.max.ckpt"
  use_teacher_annealing = true 
  auxiliary_train_path = train.english.arrau.plural.jsonlines
  auxiliary_train_type = coref
}

#best model that uses one auxiliary corpus
plural_annealing_single_coref = ${baseline}{
  use_teacher_annealing = true 
  auxiliary_train_path = train.english.arrau.plural.jsonlines
  auxiliary_train_type = coref
}



